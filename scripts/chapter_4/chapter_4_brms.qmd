---
title: "Chapter 4 BRMs"
format: html
---
```{r}
library(tidyverse)
library(patchwork)
```


# 4 Geocentric Models
## 4.1 Why normal distributions are normal
### 4.1.1 Normal by addition
Here's a way to do the simulation necessary for the plot in the top panel of Figure 4.2
```{r}
set.seed(4)

pos <-
  # make data with 100 people, 16 steps each with a starting point of `step == 0` (i.e., 17 rows per person)
  crossing(person = 1:100,
           step = 0:16) %>% 
  # for all steps above `step == 0` simulate a 'deviation'
  mutate(deviation = map_dbl(step, ~if_else(. == 0, 0, runif(1, -1, 1)))) |> 
  # after grouping by 'person', compute the cumulative sum of the deviations, then 'ungroup()'
  group_by(person) |> 
  mutate(position = cumsum(deviation)) |> 
  ungroup()

glimpse(pos)

ggplot(data = pos,
       aes(x = step, y = position, group = person)) +
  geom_vline(xintercept = c(4, 8, 16), linetype = 2) +
  geom_line(aes(color = person < 2, alpha = person < 2)) +
  scale_color_manual(values = c("skyblue4", "black")) +
  scale_alpha_manual(values = c(1/5, 1)) +
  scale_x_continuous("step number", breaks = 0:4 * 4) +
  theme(legend.position = "none")
```

Code for the bottom three plots
```{r}
# Figure 4.2.a.
p1 <-
  pos |> 
  filter(step == 4) |> 
  ggplot(aes(x = position)) +
  geom_line(stat = "density", color = "dodgerblue1") +
  labs(title = "4 steps")

# Figure 4.2.b.
p2 <-
  pos |> 
  filter(step == 8) |> 
  ggplot(aes(x = position)) +
  geom_density(color = "dodgerblue2", outline.type = "full") +
  labs(title = "8 steps")

# this is an intermediary step to get an SD value
sd <-
  pos |> 
  filter(step == 16) |> 
  summarize(sd = sd(position)) |> 
  pull(sd)

# Figure 4.2.c.
p3 <-
  pos |> 
  filter(step == 16) |> 
  ggplot(aes(x = position)) +
  stat_function(fun = dnorm,
                args = list(mean = 0, sd = sd),
                linetype = 2) +
  geom_density(color = "transparent", fill = "dodgerblue3", alpha = 1/2) +
  labs(title = "16 steps",
       y = "density")

# combine the ggplots
(p1 | p2 | p3) & coord_cartesian(xlim = c(-6, 6))
```

### 4.1.2 Normal by multiplication
```{r}
set.seed(4)

prod(1 + runif(12, 0, .1))

set.seed(4)

tibble(a = 1,
       b = runif(12, 0, .1)) |> 
  mutate(c = a + b) |> 
  summarize(p = prod(c))

set.seed

growth <-
  tibble(growth = map_dbl(1:10000, ~prod(1 + runif(12, min = 0, max = .1))))

ggplot(data = growth, aes(x = growth)) + geom_density()
```

The smaller the effect of each locus, the better this additive approximation will be
```{r}
set.seed(4)

samples <-
  tibble(big = map_dbl(1:10000, ~prod(1 + runif(12, min = 0, max = .5))),
         small = map_dbl(1:10000, ~prod(1 + runif(12, min = 0, max = .01))))

# wrangle
samples |> 
  pivot_longer(everything(), values_to = "samples") |> 
  
  # plot
  ggplot(aes(x = samples)) +
  geom_density(fill = "black") +
  facet_wrap(~ name, scales = "free")
```

### 4.1.3 Normal by log-multiplication
Instead of saving our tibble, we'll just feed it directly into our plot
```{r}
samples |> 
  mutate(log_big = log(big)) |> 
  
  ggplot(aes(x = log_big)) +
  geom_density(fill = "grey33") +
  xlab("the log of the big")
```

### 4.1.4 Using Gaussian distributions
#### 4.1.4.4 Overthinking: Gaussian distribution
```{r}
tibble(y = seq(from = -1, to = 1, by = .01),
       mu = 0,
       sigma = .1) |> 
  # compute p(y) with a hand-made Gaussian probability density function
  mutate(p = (1 / sqrt(2 * pi * sigma)) * exp(-((y - mu)^2 / (2 * sigma^2)))) |> 
  
  ggplot(aes(x = y, y = p)) +
  geom_line() +
  ylab(expression(italic(p)(italic("y|") * mu == 0*","~sigma == 0.1)))
```

## 4.2 A language for describing models
### 4.2.1 Re-describing the globe tossing model
#### 4.2.1.1 Overthinking: From model definition to Bayes' theorem
```{r}
# how many 'p_grid' points would you like?
n_points <- 100

d <-
  tibble(p_grid = seq(from = 0, to = 1, length.out = n_points),
         w = 6,
         n = 9) |> 
  mutate(prior = dunif(p_grid, min = 0, max = 1),
         likelihood = dbinom(w, size = n, prob = p_grid)) |> 
  mutate(posterior = likelihood * prior / sum(likelihood * prior))

head(d)
```

Plot of prior, likelihood, and posterior
```{r}
d |> 
  pivot_longer(prior:posterior) |> 
  # this line allows us to dictate the order in which the panels will appear
  mutate(name = factor(name, levels = c("prior", "likelihood", "posterior"))) |> 
  
  ggplot(aes(x = p_grid, y = value, fill = name)) +
  geom_area() +
  scale_fill_manual(values = c("blue", "red", "purple")) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(legend.position = "none") +
  facet_wrap(~ name, scales = "free")
```

## 4.3 A Gaussian model of height
### 4.3.1 The data
```{r}
library(rethinking)
data(Howell1)
d <- Howell1

rm(Howell1)
detach(package:rethinking, unload = T)
library(brms)
```

```{r}
d |> 
  pivot_longer(everything()) |> 
  mutate(name = factor(name, levels = c("height", "weight", "age", "male"))) |> 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 10) +
  facet_wrap(~ name, scales = "free", ncol = 1)
```
Tiny histogram
```{r}
sparks <- c("\u2581", "\u2582", "\u2583", "\u2585", "\u2587")

histospark <- function(x, width = 10){
  bins <- graphics::hist(x, breaks = width, plot = FALSE)
  
  factor <- cut(
    bins$counts / max(bins$counts),
    breaks = seq(0, 1, length = length(sparks) + 1),
    labels = sparks,
    include.lowest = TRUE
  )
  
  paste0(factor, collapse = "")
}
```

Adult only data frame
```{r}
d2 <-
  d |> 
  filter(age >= 18)

d2 |> count()
```

### 4.3.2 The model
```{r}
p1 <-
  tibble(x = seq(from = 100, to = 250, by = .1)) |> 
  
  ggplot(aes(x = x, y = dnorm(x, mean = 178, sd = 20))) +
  geom_line() +
  scale_x_continuous(breaks = seq(from = 100, to = 250, by = 75)) +
  labs(title = "mu ~ dnorm(178, 20)",
       y = "density")

p1
```

```{r}
p2 <-
  tibble(x = seq(from = -10, to = 60, by = .1)) |> 
  
  ggplot(aes(x = x, y = dunif(x, min = 0, max = 50))) +
  geom_line() +
  scale_x_continuous(breaks = c(0, 50)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("sigma ~ dunif(0, 50)")

p2
```
Simulate from both priors at once to get a prior probability distribution of heights
```{r}
n <- 1e4

set.seed(4)

sim <-
  tibble(sample_mu = rnorm(n, mean = 178, sd = 20),
         sample_sigma = runif(n, min = 0, max = 50)) |> 
  mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma))

p3 <- sim |> 
  ggplot(aes(x = height)) +
  geom_density(fill = "grey33") +
  scale_x_continuous(breaks = c(0, 73, 178, 283)) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)") +
  theme(panel.grid = element_blank())

p3
```

The x-axis breaks on the plot is intentional. To compute the mean and 3 standard deviations above and below you might do this
```{r}
sim |> 
  summarise(ll = mean(height) - sd(height) * 3,
            mean = mean(height),
            ul = mean(height) + sd(height) * 3) |> 
  mutate_all(round, digits = 1)
```

```{r}
set.seed(4)

sim <-
  tibble(sample_mu = rnorm(n, mean = 178, sd = 100),
         sample_sigma = runif(n, min = 0, max = 50)) |> 
  mutate(height = rnorm(n, mean = sample_mu, sd = sample_sigma))

breaks <-
  c(mean(sim$height) - 3 * sd(sim$height), 0, mean(sim$height), mean(sim$height) + 3 * sd(sim$height)) |> 
  round(digits = 0)

# this is just for aesthetics
text <-
  tibble(height = 272 - 25,
         y = .0013,
         label = "tallest man",
         angle = 90)

# plot
p4 <-
  sim |> 
  ggplot(aes(x = height)) +
  geom_density(fill = "black", linewidth = 0) +
  geom_vline(xintercept = 0, color = "grey92") +
  geom_vline(xintercept = 272, color = "grey92", linetype = 3) +
  geom_text(data = text,
            aes(y = y, label = label, angle = angle),
            color = "grey92") +
  scale_x_continuous(breaks = breaks) +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("height ~ dnorm(mu, sigma)\nmu ~ dnorm(178, 100)") +
  theme(panel.grid = element_blank())

p4
```

Combine all four charts
```{r}
(p1 + xlab("mu") | p2 + xlab("sigma")) / (p3 | p4)
```

On page 84, McElreath said his prior simulation indicated 4% of the heights would be below zero. Here’s how we might determe that percentage for our simulation.
```{r}
sim |> 
  count(height < 0) |> 
  mutate(percent = 100 * n / sum(n))

sim |> 
  count(height < 272) |> 
  mutate(percent = 100 * n / sum(n))
```

### 4.3.3 Grid approximation of the posterior distribution
```{r}
n <- 200

d_grid <-
  # we'll accomplish with 'tidyr::crossing()' what McElreath did with base R 'expand_grid()'
  crossing(mu = seq(from = 140, to = 160, length.out = n),
           sigma = seq(from = 4, to = 9, length.out = n))

glimpse(d_grid)
```

```{r}
grid_function <- function(mu, sigma){
  dnorm(d2$height, mean = mu, sd = sigma, log = T) |> 
    sum()
}
```

Now we're ready to complete the tibble
```{r}
d_grid <-
  d_grid |> 
  mutate(log_likelihood = map2(mu, sigma, grid_function)) |> 
  unnest(log_likelihood) |> 
  mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T),
         prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) |> 
  mutate(product = log_likelihood + prior_mu + prior_sigma) |> 
  mutate(probability = exp(product - max(product)))

head(d_grid)
```

In the final d_grid, the probability vector contains the posterior probabilities across values of mu and sigma. We can make a contour plot with geom_countour()
```{r}
d_grid |> 
  ggplot(aes(x = mu, y = sigma, z = probability)) +
  geom_contour() +
  labs(x = expression(mu),
       y = expression(sigma)) +
  coord_cartesian(xlim = range(d_grid$mu),
                  ylim = range(d_grid$sigma)) +
  theme(panel.grid = element_blank())
```
We'll make our heat map with geom_raster()
```{r}
d_grid |> 
  ggplot(aes(x = mu, y = sigma, fill = probability)) +
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "B") +
  labs(x = expression(mu),
       y = expression(sigma)) +
  theme(panel.grid = element_blank())
```

### 4.3.4 Sampling from the posterior
```{r}
set.seed(4)

d_grid_samples <-
  d_grid |> 
  sample_n(size = 1e4, replace = T, weight = probability)

d_grid_samples |> 
  ggplot(aes(x = mu, y = sigma)) +
  geom_point(size = .9, alpha = 1/15) +
  scale_fill_viridis_c() +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```
We can use pivot_longer() and facet_wrap() to plot the densities for both mu and sigma at once
```{r}
d_grid_samples |> 
  pivot_longer(mu:sigma) |> 
  
  ggplot(aes(x = value)) +
  geom_density(fill = "grey33") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~name, scales = "free", labeller = label_parsed)
```

We'll use the tidybayes package to compute their posterior modes and 95% HDIs
```{r}
library(tidybayes)

d_grid_samples |> 
  pivot_longer(mu:sigma) |> 
  group_by(name) |> 
  mode_hdi(value)
```

#### 4.3.4.1 Overthinking: Sample size and the normality of $\sigma$'s posterior
```{r}
set.seed(4)
(d3 <- sample(d2$height, size = 20))
```

```{r}
n <- 200

d_grid <-
  crossing(mu = seq(from = 150, to = 170, length.out = n),
           sigma = seq(from = 4, to = 20, length.out = n))

grid_function <- function(mu, sigma){
  dnorm(d3, mean = mu, sd = sigma, log = T) |> 
    sum()
}

d_grid <-
  d_grid |> 
  mutate(log_likelihood = map2_dbl(mu, sigma, grid_function)) |> 
  mutate(prior_mu = dnorm(mu, mean = 178, sd = 20, log = T),
         prior_sigma = dunif(sigma, min = 0, max = 50, log = T)) |> 
  mutate(product = log_likelihood + prior_mu + prior_sigma) |> 
  mutate(probability = exp(product - max(product)))
```

next we sample_n() and plot
```{r}
set.seed(4)

d_grid_samples <-
  d_grid |> 
  sample_n(size = 1e4, replace = T, weight = probability)

d_grid_samples |> 
  ggplot(aes(x = mu, y = sigma)) +
  geom_point(size = .9, alpha = 1/15) +
  labs(x = expression(mu[samples]),
       y = expression(sigma[samples])) +
  theme(panel.grid = element_blank())
```

Behold the updated densities
```{r}
d_grid_samples |> 
  pivot_longer(mu:sigma) |> 
  
  ggplot(aes(x = value)) +
  geom_density(fill = "grey33", linewidth = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~ name, scales = "free", labeller = label_parsed)
```

### 4.3.5 Finding the posterior distribution with brm()
```{r}
b4.1 <-
  brm(data = d2,
      family = gaussian,
      height ~ 1,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.01")
```

```{r}
plot(b4.1)
```

```{r}
print(b4.1)
```

```{r}
b4.1$fit
```

```{r}
summary(b4.1, prob = .89)
```

```{r}
b4.2 <-
  brm(data = d2,
      family = gaussian,
      height ~ 1,
      prior = c(prior(normal(178, 0.1), class = Intercept),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.02")
```

```{r}
plot(b4.2, widths = c(1, 2))
```

```{r}
summary(b4.2)
```

```{r}
rbind(summary(b4.1)$fixed,
      summary(b4.2)$fixed)
```

### 4.3.6 Sampling from a brm() fit

brms doesn't seem to have a convenience function that works the way vcov() does for rethinking. For example:
```{r}
vcov(b4.1)
```

```{r}
post <- as_draws_df(b4.1)

head(post)
```

```{r}
select(post, b_Intercept:sigma) |> cov()
```

```{r}
# variances
select(post, b_Intercept:sigma) |> 
  cov() |> 
  diag()
```


```{r}
# correlation
post |> 
  select(b_Intercept, sigma) |> 
  cor()
```

```{r}
str(post)
```


```{r}
summary(post[, 1:2])
```

```{r}
t(apply(post[, 1:2], 2, quantile, probs = c(.5, .025, .75)))
```
The base R code is compact, but somewhat opaque. Here's how to do something similar with more explicit tidyverse code
```{r}
post |> 
  pivot_longer(b_Intercept:sigma) |> 
  group_by(name) |> 
  summarize(mean = mean(value),
            sd = sd(value),
            '2.5%' = quantile(value, probs = .025),
            '97.5%' = quantile(value, probs = .975)) |> 
  mutate_if(is.numeric, round, digits = 2)
```
You can always get pretty similar information by just putting the brm() fit object into posterior_summary()
```{r}
posterior_summary(b4.1)
```

If you're willing to drop the posterior SDs, you can use tidybayes::mean_hdi(), too
```{r}
post |> 
  pivot_longer(b_Intercept:sigma) |> 
  group_by(name) |> 
  mean_qi(value)
```

Little histogram
```{r}
rbind(histospark(post$b_Intercept),
      histospark(post$sigma))
```

You can tack those output from verbose tidyverse code from a few blocks up
```{r}
post |> 
  pivot_longer(b_Intercept:sigma) |> 
  group_by(name) |> 
  summarize(mean = mean(value),
            sd = sd(value),
            '2.5%' = quantile(value, probs = .025),
            '97.5%' = quantile(value, probs = .975)) |> 
  mutate_if(is.numeric, round, digits = 2) |> 
  mutate(histospark = c(histospark(post$b_Intercept), histospark(post$sigma)))
```

#### 4.4 Linear prediction
```{r}
ggplot(data = d2,
       aes(x = weight, y = height)) +
  geom_point(shape = 1, size = 2) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

### 4.4.1 The linear model strategy
#### 4.4.1.3 Priors
```{r}
set.seed(2971)
n_lines <- 100

lines <-
  tibble(n = 1:n_lines,
         a = rnorm(n_lines, mean = 178, sd = 20),
         b = rnorm(n_lines, mean = 0, sd = 10)) |> 
  expand_grid(weight = range(d2$weight)) |> 
  mutate(height = a + b * (weight - mean(d2$weight))) |> 
  round(digits = 1)

head(lines)
```

```{r}
lines |> 
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("b ~ dnorm(0, 10)") +
  theme_classic()
```

Log-Normal(0, 1) distribution with positive slope
```{r}
set.seed(4)

tibble(b = rlnorm(1e4, mean = 0, sd = 1)) |> 
  ggplot(aes(x = b)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(0, 5)) +
  theme_classic()
```

Log normal distribution is distribution whose logarithm is normally distributted Normal(0, 1) = log(Log-Normal(0, 1))
```{r}
set.seed(4)

tibble(rnorm = rnorm(1e5, mean = 0, sd = 1),
       'log(rlognorm)' = log(rlnorm(1e5, mean = 0, sd = 1))) |> 
  pivot_longer(everything()) |> 
  
  ggplot(aes(x = value)) +
  geom_density(fill = "grey92") +
  coord_cartesian(xlim = c(-3, 3)) +
  theme_classic() +
  facet_wrap(~ name, nrow = 2)
```

Let's try use actual formulas for Log-Normal distribution
```{r}
mu <- 0
sigma <- 1

# mean
exp(mu + (sigma^2) / 2)

# sd
sqrt((exp(sigma^2) - 1) * exp(2 * mu + sigma^2))
```

confirm with simulated draws from rlnorm()
```{r}
set.seed(4)

tibble(x = rlnorm(1e7, mean = 0, sd = 1)) |> 
  summarize(mean = mean(x),
            sd = sd(x))
```

Prior predictive simulation with Log-Normal prior
```{r}
# make a tibble to annotate the plot
text <-
  tibble(weight = c(34, 43),
         height = c(0 - 25, 272 + 25),
         label = c("Embryo", "World's tallest person (272 cm)"))

# simulate
set.seed(2971)

tibble(n = 1:n_lines,
       a = rnorm(n_lines, mean = 178, sd = 20),
       b = rlnorm(n_lines, mean = 0, sd = 1)) |> 
  expand_grid(weight = range(d2$weight)) |> 
  mutate(height = a + b * (weight - mean(d2$weight))) |> 
  
  # plot
  ggplot(aes(x = weight, y = height, group = n)) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, linewidth = 1/3) +
  geom_line(alpha = 1/10) +
  geom_text(data = text,
            aes(label = label),
            size = 3) +
  coord_cartesian(ylim = c(-100, 400)) +
  ggtitle("log(b) ~ dnorm(0, 1)") +
  theme_classic()
```

### 4.4.2 Finding the posterior distribution
```{r}
d2 <-
  d2 |> 
  mutate(weight_c = weight - mean(weight))
```

Create model using brm
```{r}
b4.3 <-
  brm(data = d2,
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03")
```

```{r}
plot(b4.3, widths = c(1, 2))
```

#### 4.4.2.1 Overthinking: Logs and exps, oh my
```{r}
b4.3b <-
  brm(data = d2,
      family = gaussian,
      bf(height ~ a + exp(lb) * weight_c,
         a ~ 1,
         lb ~ 1,
         nl = TRUE),
      prior = c(prior(normal(178, 20), class = b, nlpar = a),
                prior(normal(0, 1), class = b, nlpar = lb),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03b")
```

```{r}
fixef(b4.3)["weight_c", "Estimate"]

fixef(b4.3b)["lb_Intercept", "Estimate"] |> exp()
```

### 4.4.3 Interpreting the posterior distribution
#### 4.4.3.1 Tables of marginal distributions
```{r}
posterior_summary(b4.3)[1:3, ] |> 
  round(digits = 2)
```

```{r}
vcov(b4.3) |> round(3)
```

```{r}
as_draws_df(b4.3) |> 
  select(b_Intercept:sigma) |> 
  cov() |> 
  round(digits = 3)
```

```{r}
pairs(b4.3)
```

#### Plotting posterior inference against the data
```{r}
d2 |> 
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = fixef(b4.3)[1],
              slope = fixef(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  theme_classic()
```

Note how the breaks on our  
x-axis look off. That’s because we fit the model with weight_c and we plotted the points in that metric, too. Since we computed weight_c by subtracting the mean of weight from the data, we can adjust the  
x-axis break point labels by simply adding that value back.
```{r}
labels <-
  c(-10, 0, 10) + mean(d2$weight) |> 
  round(digits = 0)

d2 |> 
  ggplot(aes(x = weight_c, y = height)) +
  geom_abline(intercept = fixef(b4.3)[1],
              slope = fixef(b4.3)[2]) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) +
  theme_bw() +
  theme(panel.grid = element_blank())
```

#### 4.4.3.3 Adding uncertainty around the mean
```{r}
post <- as_draws_df(b4.3)

post |> slice(1:5)
```

Here are the four models leading up to McElreath's Figure 4.7
```{r}
N <- 10

b4.3_010 <-
  brm(data = d2 |> 
        slice(1:N), # note our tricky use of 'N' and 'slice()'
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_010")

N <- 50

b4.3_050 <- 
  brm(data = d2 %>%
        slice(1:N), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_050")

N <- 150

b4.3_150 <- 
  brm(data = d2 %>%
        slice(1:N), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_150")

N <- 352

b4.3_352 <- 
  brm(data = d2 %>%
        slice(1:N), 
      family = gaussian,
      height ~ 1 + weight_c,
      prior = c(prior(normal(178, 20), class = Intercept),
                prior(lognormal(0, 1), class = b),
                prior(uniform(0, 50), class = sigma, ub = 50)),
      iter = 2000, warmup = 1000, chains = 4, cores = 4,
      seed = 4,
      file = "fits/b04.03_352")
```

```{r}
plot(b4.3_010)
print(b4.3_010)

plot(b4.3_050)
print(b4.3_050)

plot(b4.3_150)
print(b4.3_150)

plot(b4.3_352)
print(b4.3_352)
```

Put the chains of each model into data frames
```{r}
post010 <- as_draws_df(b4.3_010)
post050 <- as_draws_df(b4.3_050)
post150 <- as_draws_df(b4.3_150)
post352 <- as_draws_df(b4.3_352)
```

```{r}
p1 <- 
  ggplot(data =  d2[1:10, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post010 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 10")

p2 <-
  ggplot(data =  d2[1:50, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post050 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 50")

p3 <-
  ggplot(data =  d2[1:150, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post150 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 150")

p4 <- 
  ggplot(data =  d2[1:352, ], 
         aes(x = weight_c, y = height)) +
  geom_abline(data = post352 %>% slice(1:20),
              aes(intercept = b_Intercept, slope = b_weight_c),
              linewidth = 1/3, alpha = .3) +
  geom_point(shape = 1, size = 2, color = "royalblue") +
  coord_cartesian(xlim = range(d2$weight_c),
                  ylim = range(d2$height)) +
  labs(subtitle = "N = 352")
```

combine the ggplots with patchwork syntax
```{r}
(p1 + p2 + p3 + p4) &
  scale_x_continuous("weight",
                     breaks = c(-10, 0, 10),
                     labels = labels) &
  theme_classic()
```

